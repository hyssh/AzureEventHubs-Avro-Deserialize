{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Event Hub Avro desirialization\r\n",
        "\r\n",
        "hyssh@microsoft.com\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reference code sample \r\n",
        "\r\n",
        "Ref 1: https://stackoverflow.com/questions/72417454/azure-schema-registry-spark-structured-streaming-kafka-eventhub-compatibilit\r\n",
        "\r\n",
        "Ref 2: https://spark.apache.org/docs/latest/sql-data-sources-avro.html#to_avro-and-from_avro"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\r\n",
        "\r\n",
        "from azure.identity import DefaultAzureCredential\r\n",
        "\r\n",
        "# from azure.eventhub import EventHubConsumerClient\r\n",
        "from azure.schemaregistry import SchemaRegistryClient\r\n",
        "from azure.schemaregistry.serializer.avroserializer import AvroSerializer\r\n",
        "\r\n",
        "\r\n",
        "# from pyspark.sql.avro.functions import from_avro, to_avro\r\n"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Environment variables\r\n",
        "\r\n",
        "Get the variables from [Spark configuration](https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-azure-create-spark-configuration)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"AZURE_CLIENT_ID\"] = spark.sparkContext.environment.get(\"AZURE_CLIENT_ID\")\r\n",
        "os.environ[\"AZURE_TENANT_ID\"] = spark.sparkContext.environment.get(\"AZURE_TENANT_ID\")\r\n",
        "os.environ[\"AZURE_CLIENT_SECRET\"] = spark.sparkContext.environment.get(\"AZURE_CLIENT_SECRET\")\r\n",
        "os.environ[\"EVENT_HUB_CONN_STR\"] = spark.sparkContext.environment.get(\"EVENT_HUB_CONN_STR\")\r\n",
        "os.environ[\"EVENT_HUB_CONN_STR_ENT\"] = spark.sparkContext.environment.get(\"EVENT_HUB_CONN_STR_ENT\")\r\n",
        "os.environ[\"EVENT_HUB_CONN_STR_LISTEN\"] = spark.sparkContext.environment.get(\"EVENT_HUB_CONN_STR_LISTEN\")\r\n",
        "\r\n",
        "SCHEMAREGISTRY_FULLY_QUALIFIED_NAMESPACE = \"shin-eventhub-ns.servicebus.windows.net\"\r\n",
        "EVENTHUB_NAME=\"transactions\"\r\n"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define connection string"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ehConf = {}\r\n",
        "\r\n",
        "# For versions before 2.3.15, set the connection string without encryption\r\n",
        "# ehConf['eventhubs.connectionString'] = os.environ[\"EVENT_HUB_CONN_STR\"]\r\n",
        "\r\n",
        "# For 2.3.15 version and above, the configuration dictionary requires that connection string be encrypted.\r\n",
        "ehConf['eventhubs.connectionString'] = sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(os.environ[\"EVENT_HUB_CONN_STR_ENT\"])\r\n",
        "\r\n",
        "# Confrim the consumer group from Event Hub\r\n",
        "# https://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-features#consumer-groups\r\n",
        "ehConf['eventhubs.consumerGroup'] = \"spark\""
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create UDF\r\n",
        "\r\n",
        "Create a UDF to desrialize Avro message using Avro Serializer that supported SchemaRegistry"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StringType\r\n",
        "from pyspark.sql.functions import udf\r\n",
        "\r\n",
        "def deserializeBody(encodedBody):    \r\n",
        "\r\n",
        "    token_credential = DefaultAzureCredential()\r\n",
        "\r\n",
        "    schema_registry_client = SchemaRegistryClient(\"shin-eventhub-ns.servicebus.windows.net\", token_credential)\r\n",
        "    avro_serializer = AvroSerializer(client=schema_registry_client, group_name=\"tranxs\")\r\n",
        "\r\n",
        "    return avro_serializer.deserialize(encodedBody)\r\n",
        "\r\n",
        "deserializedBody_udf = udf(deserializeBody, StringType())"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Optional test) Get schema from schema SchemaRegistry\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_credential = DefaultAzureCredential()\r\n",
        "\r\n",
        "schema_registry_client = SchemaRegistryClient(SCHEMAREGISTRY_FULLY_QUALIFIED_NAMESPACE, token_credential)\r\n",
        "jsonFormatSchema = schema_registry_client.get_schema(\"71e03f67bd694b30b6dad6dae1fb8d86\") \r\n",
        "print(jsonFormatSchema)\r\n",
        "\r\n",
        "# bytes_payload = b\"\".join(b for b in event.body)\r\n",
        "# deserialized_data = avro_serializer.deserialize(bytes_payload)"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read streaming data from Event Hubs"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple batch query\r\n",
        "df = spark.readStream.format(\"eventhubs\").options(**ehConf).load()\r\n",
        "\r\n",
        "# df = df.outputMode(\"append\").format(\"console\").start().awaitTermination()\r\n",
        "df.printSchema()"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write streaming data to ADLS as Parquet"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save stream as parquet\r\n",
        "ds1 = df.writeStream.format(\"parquet\")\\\r\n",
        "    .option(\"path\", \"abfss://dev-synapse@hyundevsynapsestorage.dfs.core.windows.net/streaming\")\\\r\n",
        "    .option(\"checkpointLocation\", \"abfss://dev-synapse@hyundevsynapsestorage.dfs.core.windows.net/streaming_checkpoint\")\\\r\n",
        "    .trigger(processingTime='10 seconds')\\\r\n",
        "    .start()\r\n",
        "\r\n",
        "## The result wont be seen in the output of this cell\r\n",
        "## \r\n",
        "ds2 = df.select(\"body\").writeStream.outputMode(\"append\").format(\"console\").start()#.awaitTermination()\r\n",
        "\r\n",
        "# ds.start().awaitTermination()\r\n",
        "#.writeStream.outputMode(\"append\").format(\"console\").start().awaitTermination()\r\n"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(ds1.status)\r\n",
        "print(ds2.status)"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(ds1.stop())\r\n",
        "print(ds2.stop())"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filedf = spark.read.load('abfss://dev-synapse@hyundevsynapsestorage.dfs.core.windows.net/streaming/*.parquet', format='parquet')\r\n",
        "print(filedf.count())"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The messages are saved\r\n",
        "\r\n",
        "The column body contains the unreadable raw data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filedf.show()"
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test UDF 1\r\n",
        "\r\n",
        "Use UDF to deserialize body in the file"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filedf = filedf.withColumn(\"decodedBody\", deserializedBody_udf(filedf[\"body\"]))\r\n",
        "filedf.show()"
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test 2. Use udf before wright data in ADLS\r\n",
        "\r\n",
        "In stread of dederiailze body reading data that saved in ADLS, we can try to deserialize body right before written in ADLS"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# see result in the console\r\n",
        "# output = df.select(\"value\")\r\n",
        "# output.printSchema()\r\n",
        "\r\n",
        "# df.writeStream.outputMode(\"append\").format(\"concole\").option(\"truncate\", False).start().awaitTermination()"
      ],
      "outputs": [],
      "execution_count": 47,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ds3 = df.select(\"body\")#writeStream.outputMode(\"append\").format(\"console\").start().awaitTermination(10)\r\n",
        "# ds3 = ds3.withColumn(\"decodedBody\", deserializedBody_udf(ds3[\"body\"]))\r\n",
        "# ds3.writeStream.outputMode(\"append\").format(\"console\").start().awaitTermination(300)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use `deserializeBody_udf` to deserialize the body and save the result as Parquet"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds4 = df.withColumn(\"decodedBody\", deserializedBody_udf(df[\"body\"])).writeStream.format(\"parquet\")\\\r\n",
        "    .option(\"path\", \"abfss://dev-synapse@hyundevsynapsestorage.dfs.core.windows.net/streamingAfterdecode\")\\\r\n",
        "    .option(\"checkpointLocation\", \"abfss://dev-synapse@hyundevsynapsestorage.dfs.core.windows.net/streamingAfterdecode_checkpoint\")\\\r\n",
        "    .start()\r\n",
        "    # .trigger(processingTime='10 seconds')\\\r\n",
        "    # .start()\r\n"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(ds4.status)"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### See the result"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "afterDecodedf = spark.read.load('abfss://dev-synapse@hyundevsynapsestorage.dfs.core.windows.net/streamingAfterdecode/*.parquet', format='parquet')\r\n",
        "print(afterDecodedf.count())"
      ],
      "outputs": [],
      "execution_count": 27,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "afterDecodedf.count()"
      ],
      "outputs": [],
      "execution_count": 28,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "afterDecodedf.select(\"body\", \"decodedBody\").show()"
      ],
      "outputs": [],
      "execution_count": 29,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds4.status"
      ],
      "outputs": [],
      "execution_count": 30,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds4.stop()"
      ],
      "outputs": [],
      "execution_count": 31,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## End of Notebook"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}